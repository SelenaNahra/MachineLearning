{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SelenaNahra/MachineLearning/blob/main/HW6Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fpW56Uif21o",
        "outputId": "e191c36a-ef94-40db-f4aa-ed8a5cc0a453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch: 0, Loss: 1.8211444105638568\n",
            "Epoch: 1, Loss: 1.69657650399391\n",
            "Epoch: 2, Loss: 1.6390247253505774\n",
            "Epoch: 3, Loss: 1.594413316615707\n",
            "Epoch: 4, Loss: 1.5535234738798702\n",
            "Epoch: 5, Loss: 1.5160720044999476\n",
            "Epoch: 6, Loss: 1.4800951703430136\n",
            "Epoch: 7, Loss: 1.4443244900544892\n",
            "Epoch: 8, Loss: 1.4097590531839435\n",
            "Epoch: 9, Loss: 1.3763323203681985\n",
            "Epoch: 10, Loss: 1.342907277748103\n",
            "Epoch: 11, Loss: 1.3103861708165434\n",
            "Epoch: 12, Loss: 1.2774546705853298\n",
            "Epoch: 13, Loss: 1.2426080597788476\n",
            "Epoch: 14, Loss: 1.2088662193101996\n",
            "Epoch: 15, Loss: 1.1774098034709921\n",
            "Epoch: 16, Loss: 1.143210605282308\n",
            "Epoch: 17, Loss: 1.1098749460009358\n",
            "Epoch: 18, Loss: 1.0784469810135835\n",
            "Epoch: 19, Loss: 1.0441386447385754\n",
            "Epoch: 20, Loss: 1.013659429367241\n",
            "Epoch: 21, Loss: 0.9821354054733921\n",
            "Epoch: 22, Loss: 0.9494926063605892\n",
            "Epoch: 23, Loss: 0.9179364121173654\n",
            "Epoch: 24, Loss: 0.8860048215712428\n",
            "Epoch: 25, Loss: 0.8559006150726163\n",
            "Epoch: 26, Loss: 0.8258295338172132\n",
            "Epoch: 27, Loss: 0.7968478074768925\n",
            "Epoch: 28, Loss: 0.7677306726841671\n",
            "Epoch: 29, Loss: 0.7388004681185993\n",
            "Epoch: 30, Loss: 0.7106305231599856\n",
            "Epoch: 31, Loss: 0.6831106674640685\n",
            "Epoch: 32, Loss: 0.6568395654335046\n",
            "Epoch: 33, Loss: 0.631850834926376\n",
            "Epoch: 34, Loss: 0.6052749589123689\n",
            "Epoch: 35, Loss: 0.5831994325913432\n",
            "Epoch: 36, Loss: 0.5570275519052734\n",
            "Epoch: 37, Loss: 0.5340483837267932\n",
            "Epoch: 38, Loss: 0.5124910191806686\n",
            "Epoch: 39, Loss: 0.4919304410210046\n",
            "Epoch: 40, Loss: 0.47157298882141746\n",
            "Epoch: 41, Loss: 0.45138245771455643\n",
            "Epoch: 42, Loss: 0.4304472040718474\n",
            "Epoch: 43, Loss: 0.41197495947560997\n",
            "Epoch: 44, Loss: 0.39391529266639136\n",
            "Epoch: 45, Loss: 0.37710218870883705\n",
            "Epoch: 46, Loss: 0.3599553540958773\n",
            "Epoch: 47, Loss: 0.3433301623177041\n",
            "Epoch: 48, Loss: 0.3281755727499037\n",
            "Epoch: 49, Loss: 0.31346034092823866\n",
            "Epoch: 50, Loss: 0.2991106079514984\n",
            "Epoch: 51, Loss: 0.2855905002683325\n",
            "Epoch: 52, Loss: 0.271649676896727\n",
            "Epoch: 53, Loss: 0.25963014124147116\n",
            "Epoch: 54, Loss: 0.24799911457749887\n",
            "Epoch: 55, Loss: 0.2364814974119901\n",
            "Epoch: 56, Loss: 0.2265097349501022\n",
            "Epoch: 57, Loss: 0.21590283474006006\n",
            "Epoch: 58, Loss: 0.20714489817428772\n",
            "Epoch: 59, Loss: 0.19691084791213045\n",
            "Epoch: 60, Loss: 0.18732456421798757\n",
            "Epoch: 61, Loss: 0.17975986550759782\n",
            "Epoch: 62, Loss: 0.17201543539343284\n",
            "Epoch: 63, Loss: 0.1644846296028408\n",
            "Epoch: 64, Loss: 0.15847580751304127\n",
            "Epoch: 65, Loss: 0.15085739992044467\n",
            "Epoch: 66, Loss: 0.14425454636478363\n",
            "Epoch: 67, Loss: 0.1376442132741594\n",
            "Epoch: 68, Loss: 0.13147414399935003\n",
            "Epoch: 69, Loss: 0.12865279177608696\n",
            "Epoch: 70, Loss: 0.12105676127821588\n",
            "Epoch: 71, Loss: 0.11650418083343055\n",
            "Epoch: 72, Loss: 0.11297659278678163\n",
            "Epoch: 73, Loss: 0.10837236413127169\n",
            "Epoch: 74, Loss: 0.10355571429233265\n",
            "Epoch: 75, Loss: 0.09985676240128324\n",
            "Epoch: 76, Loss: 0.09604165643987143\n",
            "Epoch: 77, Loss: 0.09229716287015954\n",
            "Epoch: 78, Loss: 0.08871787794105843\n",
            "Epoch: 79, Loss: 0.0855367142668999\n",
            "Epoch: 80, Loss: 0.08174805813814368\n",
            "Epoch: 81, Loss: 0.07923822933831788\n",
            "Epoch: 82, Loss: 0.07701163287834285\n",
            "Epoch: 83, Loss: 0.07357881004776796\n",
            "Epoch: 84, Loss: 0.0714646388712289\n",
            "Epoch: 85, Loss: 0.06907286876074188\n",
            "Epoch: 86, Loss: 0.06700350475185515\n",
            "Epoch: 87, Loss: 0.06485068097310451\n",
            "Epoch: 88, Loss: 0.06338086993435917\n",
            "Epoch: 89, Loss: 0.06134941876696809\n",
            "Epoch: 90, Loss: 0.05886984021043229\n",
            "Epoch: 91, Loss: 0.057140675771152574\n",
            "Epoch: 92, Loss: 0.055657500015271595\n",
            "Epoch: 93, Loss: 0.05426860899400071\n",
            "Epoch: 94, Loss: 0.05280020378787271\n",
            "Epoch: 95, Loss: 0.051539202532766726\n",
            "Epoch: 96, Loss: 0.05032154796478312\n",
            "Epoch: 97, Loss: 0.04862174287181147\n",
            "Epoch: 98, Loss: 0.047340194177825735\n",
            "Epoch: 99, Loss: 0.046165773016221996\n",
            "Training Time: 2131.0670948028564 seconds\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "\n",
        "cifar10 = datasets.CIFAR10('data', train=True, download=True)\n",
        "cifar10_val = datasets.CIFAR10('data', train=False, download=True)\n",
        "\n",
        "cifar10 = datasets.CIFAR10('data', train=True, download=False,\n",
        "                          transform=transforms.Compose([\n",
        "                              transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                                                   (0.2470, 0.2435, 0.2616))\n",
        "                          ]))\n",
        "cifar10_val = datasets.CIFAR10('data', train=False, download=False,\n",
        "                          transform=transforms.Compose([\n",
        "                              transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                                                   (0.2470, 0.2435, 0.2616))\n",
        "                          ]))\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(cifar10, batch_size=64, shuffle=True)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3072, 512),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(512, 10),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "n_epochs = 100\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs.view(imgs.size(0), -1))\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Loss: {average_loss}\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training Time: {training_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3072, 512),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(512, 10),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of Trainable Parameters: {num_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCiaO6KO8U_b",
        "outputId": "4e921a5e-feff-4266-9b38-fd439cdbcfce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Trainable Parameters: 1578506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qacJRszfnSE4",
        "outputId": "10cbe5c2-d368-4bbb-d064-f0ed9f308dd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.999500\n",
            "Valadation Accuracy: 0.472500\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Training Accuracy: %f\" % (correct / total))\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Valadation Accuracy: %f\" % (correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFwpPoya02id",
        "outputId": "cbf0d108-3493-4b39-d7e9-24ed3795b4f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch: 0, Loss: 1.9230859424452038\n",
            "Epoch: 1, Loss: 1.750762040505324\n",
            "Epoch: 2, Loss: 1.6878848981369488\n",
            "Epoch: 3, Loss: 1.6431334364749586\n",
            "Epoch: 4, Loss: 1.6044160633745705\n",
            "Epoch: 5, Loss: 1.567161771190136\n",
            "Epoch: 6, Loss: 1.530499988504688\n",
            "Epoch: 7, Loss: 1.4937528612668558\n",
            "Epoch: 8, Loss: 1.4578733104269217\n",
            "Epoch: 9, Loss: 1.4192542834659976\n",
            "Epoch: 10, Loss: 1.3807745983685984\n",
            "Epoch: 11, Loss: 1.3406756863264782\n",
            "Epoch: 12, Loss: 1.299565557018875\n",
            "Epoch: 13, Loss: 1.255295580534069\n",
            "Epoch: 14, Loss: 1.213007082414749\n",
            "Epoch: 15, Loss: 1.1687514728597364\n",
            "Epoch: 16, Loss: 1.1232031371130053\n",
            "Epoch: 17, Loss: 1.073126506012724\n",
            "Epoch: 18, Loss: 1.0273337415264696\n",
            "Epoch: 19, Loss: 0.9751095366295036\n",
            "Epoch: 20, Loss: 0.9279818642322365\n",
            "Epoch: 21, Loss: 0.8736381273135505\n",
            "Epoch: 22, Loss: 0.819634699112619\n",
            "Epoch: 23, Loss: 0.7749159472906376\n",
            "Epoch: 24, Loss: 0.7315402050000017\n",
            "Epoch: 25, Loss: 0.6770808484090869\n",
            "Epoch: 26, Loss: 0.6286274586873286\n",
            "Epoch: 27, Loss: 0.5911628242267672\n",
            "Epoch: 28, Loss: 0.5438609538633196\n",
            "Epoch: 29, Loss: 0.48968450088635124\n",
            "Epoch: 30, Loss: 0.45354969050649485\n",
            "Epoch: 31, Loss: 0.413081678473736\n",
            "Epoch: 32, Loss: 0.3730107483542179\n",
            "Epoch: 33, Loss: 0.3398351898736051\n",
            "Epoch: 34, Loss: 0.30367744411996866\n",
            "Epoch: 35, Loss: 0.2799628112665223\n",
            "Epoch: 36, Loss: 0.2443958970782397\n",
            "Epoch: 37, Loss: 0.2177812210891558\n",
            "Epoch: 38, Loss: 0.19714349873192474\n",
            "Epoch: 39, Loss: 0.1732251099251268\n",
            "Epoch: 40, Loss: 0.1554766240413003\n",
            "Epoch: 41, Loss: 0.14169685777200533\n",
            "Epoch: 42, Loss: 0.12529815946850936\n",
            "Epoch: 43, Loss: 0.11418635110654261\n",
            "Epoch: 44, Loss: 0.08773220081091918\n",
            "Epoch: 45, Loss: 0.08240132582138109\n",
            "Epoch: 46, Loss: 0.05892709968135218\n",
            "Epoch: 47, Loss: 0.08200470429709386\n",
            "Epoch: 48, Loss: 0.05071822756572681\n",
            "Epoch: 49, Loss: 0.04024135215920122\n",
            "Epoch: 50, Loss: 0.032718562284636946\n",
            "Epoch: 51, Loss: 0.04899889035531513\n",
            "Epoch: 52, Loss: 0.03616713515609084\n",
            "Epoch: 53, Loss: 0.024696450123129904\n",
            "Epoch: 54, Loss: 0.020349340206083588\n",
            "Epoch: 55, Loss: 0.01591607135222734\n",
            "Epoch: 56, Loss: 0.016535367747611555\n",
            "Epoch: 57, Loss: 0.01960773308120687\n",
            "Epoch: 58, Loss: 0.015503643764792692\n",
            "Epoch: 59, Loss: 0.03580876844524242\n",
            "Epoch: 60, Loss: 0.026118868097300876\n",
            "Epoch: 61, Loss: 0.042500984026690766\n",
            "Epoch: 62, Loss: 0.011739361784992563\n",
            "Epoch: 63, Loss: 0.0067963635208337665\n",
            "Epoch: 64, Loss: 0.005225270801959821\n",
            "Epoch: 65, Loss: 0.0046809300624877405\n",
            "Epoch: 66, Loss: 0.00457451366530636\n",
            "Epoch: 67, Loss: 0.0034452384219283375\n",
            "Epoch: 68, Loss: 0.0032609457515127708\n",
            "Epoch: 69, Loss: 0.002982983679470161\n",
            "Epoch: 70, Loss: 0.002823684064140234\n",
            "Epoch: 71, Loss: 0.0026581184835830596\n",
            "Epoch: 72, Loss: 0.0025186720392023645\n",
            "Epoch: 73, Loss: 0.0024181923908217218\n",
            "Epoch: 74, Loss: 0.002337460549931754\n",
            "Epoch: 75, Loss: 0.002245248871637494\n",
            "Epoch: 76, Loss: 0.0021672076285135507\n",
            "Epoch: 77, Loss: 0.0020912058034088807\n",
            "Epoch: 78, Loss: 0.0020330679602027796\n",
            "Epoch: 79, Loss: 0.0019622791802351033\n",
            "Epoch: 80, Loss: 0.0019042263341331593\n",
            "Epoch: 81, Loss: 0.001855474997513101\n",
            "Epoch: 82, Loss: 0.0018040539980109763\n",
            "Epoch: 83, Loss: 0.0017616250087022353\n",
            "Epoch: 84, Loss: 0.0017024655457939762\n",
            "Epoch: 85, Loss: 0.0016558433954646844\n",
            "Epoch: 86, Loss: 0.0016243295477437632\n",
            "Epoch: 87, Loss: 0.0015836246450856813\n",
            "Epoch: 88, Loss: 0.0015410465258888213\n",
            "Epoch: 89, Loss: 0.001504474201911341\n",
            "Epoch: 90, Loss: 0.0014743366751514967\n",
            "Epoch: 91, Loss: 0.001440822899959686\n",
            "Epoch: 92, Loss: 0.0014131297514436867\n",
            "Epoch: 93, Loss: 0.001381224706037151\n",
            "Epoch: 94, Loss: 0.0013545537688275156\n",
            "Epoch: 95, Loss: 0.0013283200934469757\n",
            "Epoch: 96, Loss: 0.0013029028894826341\n",
            "Epoch: 97, Loss: 0.0012742341277213372\n",
            "Epoch: 98, Loss: 0.001251690596451654\n",
            "Epoch: 99, Loss: 0.0012288373667815858\n",
            "Epoch: 100, Loss: 0.0012059539143839265\n",
            "Epoch: 101, Loss: 0.00118493837046275\n",
            "Epoch: 102, Loss: 0.0011640940111039965\n",
            "Epoch: 103, Loss: 0.0011467020812770709\n",
            "Epoch: 104, Loss: 0.001125310042538368\n",
            "Epoch: 105, Loss: 0.0011045970464074065\n",
            "Epoch: 106, Loss: 0.0010894574582978101\n",
            "Epoch: 107, Loss: 0.0010727979007589122\n",
            "Epoch: 108, Loss: 0.00105368595528196\n",
            "Epoch: 109, Loss: 0.001039240997933123\n",
            "Epoch: 110, Loss: 0.0010222092608336473\n",
            "Epoch: 111, Loss: 0.0010058866523336048\n",
            "Epoch: 112, Loss: 0.000992377689915121\n",
            "Epoch: 113, Loss: 0.000977594337602982\n",
            "Epoch: 114, Loss: 0.0009639978033962096\n",
            "Epoch: 115, Loss: 0.0009496262538350424\n",
            "Epoch: 116, Loss: 0.0009360729636775706\n",
            "Epoch: 117, Loss: 0.0009228887020513806\n",
            "Epoch: 118, Loss: 0.0009115771024652859\n",
            "Epoch: 119, Loss: 0.0008986815304049979\n",
            "Epoch: 120, Loss: 0.0008854342198334015\n",
            "Epoch: 121, Loss: 0.0008746810222301832\n",
            "Epoch: 122, Loss: 0.0008642229561508178\n",
            "Epoch: 123, Loss: 0.0008506497279341902\n",
            "Epoch: 124, Loss: 0.0008422857754246887\n",
            "Epoch: 125, Loss: 0.0008313963372085263\n",
            "Epoch: 126, Loss: 0.0008212538071550176\n",
            "Epoch: 127, Loss: 0.0008106917702465477\n",
            "Epoch: 128, Loss: 0.0008012108820597129\n",
            "Epoch: 129, Loss: 0.0007915632941660678\n",
            "Epoch: 130, Loss: 0.0007827162869599746\n",
            "Epoch: 131, Loss: 0.0007732560833343817\n",
            "Epoch: 132, Loss: 0.0007637475249226517\n",
            "Epoch: 133, Loss: 0.0007554797315061964\n",
            "Epoch: 134, Loss: 0.0007464128576264934\n",
            "Epoch: 135, Loss: 0.0007392473925557702\n",
            "Epoch: 136, Loss: 0.0007308228275231907\n",
            "Epoch: 137, Loss: 0.0007225891558543953\n",
            "Epoch: 138, Loss: 0.0007152024173380835\n",
            "Epoch: 139, Loss: 0.0007072717230201311\n",
            "Epoch: 140, Loss: 0.0006991941462475759\n",
            "Epoch: 141, Loss: 0.0006927797387567256\n",
            "Epoch: 142, Loss: 0.000684846487358365\n",
            "Epoch: 143, Loss: 0.0006779517898115132\n",
            "Epoch: 144, Loss: 0.0006708993802077549\n",
            "Epoch: 145, Loss: 0.0006646215860226342\n",
            "Epoch: 146, Loss: 0.0006580396263655918\n",
            "Epoch: 147, Loss: 0.0006511864385849265\n",
            "Epoch: 148, Loss: 0.0006449873841804144\n",
            "Epoch: 149, Loss: 0.0006378859565432524\n",
            "Epoch: 150, Loss: 0.00063146063749788\n",
            "Epoch: 151, Loss: 0.0006273661485484318\n",
            "Epoch: 152, Loss: 0.0006205957899715089\n",
            "Epoch: 153, Loss: 0.0006144348580864809\n",
            "Epoch: 154, Loss: 0.000609409232722724\n",
            "Epoch: 155, Loss: 0.0006033700067669634\n",
            "Epoch: 156, Loss: 0.0005981536624391023\n",
            "Epoch: 157, Loss: 0.0005932562148851722\n",
            "Epoch: 158, Loss: 0.0005879068446894655\n",
            "Epoch: 159, Loss: 0.0005817915442238307\n",
            "Epoch: 160, Loss: 0.0005765466555140798\n",
            "Epoch: 161, Loss: 0.0005725041095329963\n",
            "Epoch: 162, Loss: 0.0005667042289384166\n",
            "Epoch: 163, Loss: 0.000561859677670776\n",
            "Epoch: 164, Loss: 0.0005572324323862711\n",
            "Epoch: 165, Loss: 0.0005522065810026134\n",
            "Epoch: 166, Loss: 0.0005479048756088185\n",
            "Epoch: 167, Loss: 0.0005432327488309863\n",
            "Epoch: 168, Loss: 0.0005383650712205021\n",
            "Epoch: 169, Loss: 0.0005346511182782557\n",
            "Epoch: 170, Loss: 0.0005299176659707166\n",
            "Epoch: 171, Loss: 0.0005258378825317759\n",
            "Epoch: 172, Loss: 0.0005217290713342831\n",
            "Epoch: 173, Loss: 0.0005176208885859988\n",
            "Epoch: 174, Loss: 0.0005133098632301492\n",
            "Epoch: 175, Loss: 0.0005090444860414214\n",
            "Epoch: 176, Loss: 0.0005053215375964475\n",
            "Epoch: 177, Loss: 0.0005013405286130564\n",
            "Epoch: 178, Loss: 0.0004966706098185357\n",
            "Epoch: 179, Loss: 0.000493763415912545\n",
            "Epoch: 180, Loss: 0.0004901746973854458\n",
            "Epoch: 181, Loss: 0.0004865072216973176\n",
            "Epoch: 182, Loss: 0.00048266605639596806\n",
            "Epoch: 183, Loss: 0.00047893734948645293\n",
            "Epoch: 184, Loss: 0.000475661582289659\n",
            "Epoch: 185, Loss: 0.00047175689266793327\n",
            "Epoch: 186, Loss: 0.00046890457634173353\n",
            "Epoch: 187, Loss: 0.0004651396154754145\n",
            "Epoch: 188, Loss: 0.0004621041757638192\n",
            "Epoch: 189, Loss: 0.00045843692183422516\n",
            "Epoch: 190, Loss: 0.0004556649435988015\n",
            "Epoch: 191, Loss: 0.0004527775305838388\n",
            "Epoch: 192, Loss: 0.0004490788378368687\n",
            "Epoch: 193, Loss: 0.00044596488053119643\n",
            "Epoch: 194, Loss: 0.00044236233533994843\n",
            "Epoch: 195, Loss: 0.00044013266606480263\n",
            "Epoch: 196, Loss: 0.00043680934823336096\n",
            "Epoch: 197, Loss: 0.0004340279891299765\n",
            "Epoch: 198, Loss: 0.0004310044095642469\n",
            "Epoch: 199, Loss: 0.0004281505221336344\n",
            "Epoch: 200, Loss: 0.00042522366498010426\n",
            "Epoch: 201, Loss: 0.0004223333743890352\n",
            "Epoch: 202, Loss: 0.0004196118234339363\n",
            "Epoch: 203, Loss: 0.000417051430232019\n",
            "Epoch: 204, Loss: 0.0004141867152460472\n",
            "Epoch: 205, Loss: 0.00041143767509038996\n",
            "Epoch: 206, Loss: 0.0004088112471766336\n",
            "Epoch: 207, Loss: 0.00040669484466896095\n",
            "Epoch: 208, Loss: 0.0004037423244127563\n",
            "Epoch: 209, Loss: 0.00040121061485939565\n",
            "Epoch: 210, Loss: 0.0003987413100407595\n",
            "Epoch: 211, Loss: 0.00039645688561171824\n",
            "Epoch: 212, Loss: 0.000393922421708316\n",
            "Epoch: 213, Loss: 0.0003916633814392264\n",
            "Epoch: 214, Loss: 0.00038886238002296315\n",
            "Epoch: 215, Loss: 0.00038645969636579307\n",
            "Epoch: 216, Loss: 0.00038426712792425695\n",
            "Epoch: 217, Loss: 0.00038203789358762095\n",
            "Epoch: 218, Loss: 0.00037974343499894635\n",
            "Epoch: 219, Loss: 0.0003773884653226029\n",
            "Epoch: 220, Loss: 0.00037508042253967124\n",
            "Epoch: 221, Loss: 0.00037251250663027286\n",
            "Epoch: 222, Loss: 0.00037075953496753443\n",
            "Epoch: 223, Loss: 0.00036841132110157444\n",
            "Epoch: 224, Loss: 0.0003664270242857163\n",
            "Epoch: 225, Loss: 0.00036423809518141415\n",
            "Epoch: 226, Loss: 0.00036257449581804076\n",
            "Epoch: 227, Loss: 0.0003601325757499627\n",
            "Epoch: 228, Loss: 0.0003580041774584319\n",
            "Epoch: 229, Loss: 0.00035662180937640845\n",
            "Epoch: 230, Loss: 0.0003545582506602006\n",
            "Epoch: 231, Loss: 0.0003520766846434263\n",
            "Epoch: 232, Loss: 0.00035022904992501353\n",
            "Epoch: 233, Loss: 0.0003481732560140069\n",
            "Epoch: 234, Loss: 0.0003466349786005752\n",
            "Epoch: 235, Loss: 0.0003446667582828902\n",
            "Epoch: 236, Loss: 0.00034263556684091057\n",
            "Epoch: 237, Loss: 0.00034054640107109306\n",
            "Epoch: 238, Loss: 0.00033879880178192765\n",
            "Epoch: 239, Loss: 0.0003371547021896547\n",
            "Epoch: 240, Loss: 0.00033508412134626705\n",
            "Epoch: 241, Loss: 0.0003333704681303936\n",
            "Epoch: 242, Loss: 0.0003316580666862714\n",
            "Epoch: 243, Loss: 0.00033030284510906355\n",
            "Epoch: 244, Loss: 0.0003283981069819132\n",
            "Epoch: 245, Loss: 0.0003265668981378991\n",
            "Epoch: 246, Loss: 0.0003248160646762699\n",
            "Epoch: 247, Loss: 0.00032328010125927116\n",
            "Epoch: 248, Loss: 0.00032160953874863527\n",
            "Epoch: 249, Loss: 0.00031994288089647033\n",
            "Epoch: 250, Loss: 0.0003183262629682779\n",
            "Epoch: 251, Loss: 0.0003167076265480181\n",
            "Epoch: 252, Loss: 0.00031509686474223285\n",
            "Epoch: 253, Loss: 0.00031358417385713913\n",
            "Epoch: 254, Loss: 0.00031246284686956114\n",
            "Epoch: 255, Loss: 0.0003103765726136818\n",
            "Epoch: 256, Loss: 0.000308844565884764\n",
            "Epoch: 257, Loss: 0.00030771638613606294\n",
            "Epoch: 258, Loss: 0.0003057138909034364\n",
            "Epoch: 259, Loss: 0.0003044509443232928\n",
            "Epoch: 260, Loss: 0.0003028648913718279\n",
            "Epoch: 261, Loss: 0.0003011795301588795\n",
            "Epoch: 262, Loss: 0.00029993796906963374\n",
            "Epoch: 263, Loss: 0.0002985782940481084\n",
            "Epoch: 264, Loss: 0.0002971370478191644\n",
            "Epoch: 265, Loss: 0.0002956270169944007\n",
            "Epoch: 266, Loss: 0.0002940881691161183\n",
            "Epoch: 267, Loss: 0.0002930333633246758\n",
            "Epoch: 268, Loss: 0.0002914257212113256\n",
            "Epoch: 269, Loss: 0.0002901033579517463\n",
            "Epoch: 270, Loss: 0.0002886600346695108\n",
            "Epoch: 271, Loss: 0.0002874946553686448\n",
            "Epoch: 272, Loss: 0.00028601900634654773\n",
            "Epoch: 273, Loss: 0.00028486324532199866\n",
            "Epoch: 274, Loss: 0.00028342102653385065\n",
            "Epoch: 275, Loss: 0.00028240660922956124\n",
            "Epoch: 276, Loss: 0.0002809152526803591\n",
            "Epoch: 277, Loss: 0.0002799608107597288\n",
            "Epoch: 278, Loss: 0.0002785614737675137\n",
            "Epoch: 279, Loss: 0.0002772254550800054\n",
            "Epoch: 280, Loss: 0.0002760780344614783\n",
            "Epoch: 281, Loss: 0.0002748292519005618\n",
            "Epoch: 282, Loss: 0.0002735771383467021\n",
            "Epoch: 283, Loss: 0.00027223701207407887\n",
            "Epoch: 284, Loss: 0.0002711081265202721\n",
            "Epoch: 285, Loss: 0.00026989881869044055\n",
            "Epoch: 286, Loss: 0.00026869438709202877\n",
            "Epoch: 287, Loss: 0.00026738884206891034\n",
            "Epoch: 288, Loss: 0.00026652479757610805\n",
            "Epoch: 289, Loss: 0.0002651682839302532\n",
            "Epoch: 290, Loss: 0.000264310755550622\n",
            "Epoch: 291, Loss: 0.00026284866063269643\n",
            "Epoch: 292, Loss: 0.00026194012697141196\n",
            "Epoch: 293, Loss: 0.0002607518685055668\n",
            "Epoch: 294, Loss: 0.00025960860108180194\n",
            "Epoch: 295, Loss: 0.0002587861580210869\n",
            "Epoch: 296, Loss: 0.00025762447633955013\n",
            "Epoch: 297, Loss: 0.0002564222805442877\n",
            "Epoch: 298, Loss: 0.00025528944416410503\n",
            "Epoch: 299, Loss: 0.0002542755330692145\n",
            "Training Time: 9644.558907747269 seconds\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "cifar10 = datasets.CIFAR10('data', train=True, download=True)\n",
        "cifar10_val = datasets.CIFAR10('data', train=False, download=True)\n",
        "\n",
        "cifar10 = datasets.CIFAR10('data', train=True, download=False,\n",
        "                          transform=transforms.Compose([\n",
        "                              transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                                                   (0.2470, 0.2435, 0.2616))\n",
        "                          ]))\n",
        "cifar10_val = datasets.CIFAR10('data', train=False, download=False,\n",
        "                          transform=transforms.Compose([\n",
        "                              transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                                                   (0.2470, 0.2435, 0.2616))\n",
        "                          ]))\n",
        "\n",
        "train_loader = DataLoader(cifar10, batch_size=64, shuffle=True)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3072, 1024),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(1024, 512),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(512, 128),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(128, 10),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "n_epochs = 300\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs.view(imgs.size(0), -1))\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Loss: {average_loss}\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training Time: {training_time} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3072, 1024),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(1024, 512),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(512, 128),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(128, 10),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of Trainable Parameters: {num_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8A_QZUu8mOg",
        "outputId": "4e6fda89-734a-4f5d-898d-0036e6bc4e8f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Trainable Parameters: 3738506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Training Accuracy: %f\" % (correct / total))\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Valadation Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH9KUp4jmH78",
        "outputId": "314134ba-98a2-48b8-bd23-37e8e7cc1d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 1.000000\n",
            "Valadation Accuracy: 0.463800\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxpnTMKKocwcW7ZWdRTk0e",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}