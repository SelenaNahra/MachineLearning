{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SelenaNahra/MachineLearning/blob/main/HW6Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fpW56Uif21o",
        "outputId": "439b2653-9c77-4fd3-df3e-d1cd129869f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch: 0, Loss: 1.8231855496726073\n",
            "Epoch: 1, Loss: 1.6995149032234231\n",
            "Epoch: 2, Loss: 1.6421545923823286\n",
            "Epoch: 3, Loss: 1.596505774104077\n",
            "Epoch: 4, Loss: 1.557632731659638\n",
            "Epoch: 5, Loss: 1.5181197609438006\n",
            "Epoch: 6, Loss: 1.4812352745734212\n",
            "Epoch: 7, Loss: 1.4463958375899078\n",
            "Epoch: 8, Loss: 1.410214875055396\n",
            "Epoch: 9, Loss: 1.375881762227134\n",
            "Epoch: 10, Loss: 1.34109050462313\n",
            "Epoch: 11, Loss: 1.3071723779296631\n",
            "Epoch: 12, Loss: 1.2735503498855454\n",
            "Epoch: 13, Loss: 1.2404931620563693\n",
            "Epoch: 14, Loss: 1.2077120312339509\n",
            "Epoch: 15, Loss: 1.1739798984716616\n",
            "Epoch: 16, Loss: 1.1412969691979\n",
            "Epoch: 17, Loss: 1.1081859320021041\n",
            "Epoch: 18, Loss: 1.074239082043738\n",
            "Epoch: 19, Loss: 1.0417090619311613\n",
            "Epoch: 20, Loss: 1.0106260464776813\n",
            "Epoch: 21, Loss: 0.9768748550158938\n",
            "Epoch: 22, Loss: 0.9463140256417072\n",
            "Epoch: 23, Loss: 0.9128545985349914\n",
            "Epoch: 24, Loss: 0.8834748285658219\n",
            "Epoch: 25, Loss: 0.8519839083447176\n",
            "Epoch: 26, Loss: 0.8208719236619028\n",
            "Epoch: 27, Loss: 0.791347903173293\n",
            "Epoch: 28, Loss: 0.7647803855292937\n",
            "Epoch: 29, Loss: 0.7340594194353084\n",
            "Epoch: 30, Loss: 0.7076959034899617\n",
            "Epoch: 31, Loss: 0.6809223896783331\n",
            "Epoch: 32, Loss: 0.653499297511852\n",
            "Epoch: 33, Loss: 0.6269510474122698\n",
            "Epoch: 34, Loss: 0.6027631683041678\n",
            "Epoch: 35, Loss: 0.5789276428539735\n",
            "Epoch: 36, Loss: 0.5563544750289844\n",
            "Epoch: 37, Loss: 0.5323299000711392\n",
            "Epoch: 38, Loss: 0.5098521549378514\n",
            "Epoch: 39, Loss: 0.4885597671465496\n",
            "Epoch: 40, Loss: 0.46753452849738736\n",
            "Epoch: 41, Loss: 0.44749173179002066\n",
            "Epoch: 42, Loss: 0.4279631995560263\n",
            "Epoch: 43, Loss: 0.4094008136816952\n",
            "Epoch: 44, Loss: 0.3913101009891161\n",
            "Epoch: 45, Loss: 0.37424712149840794\n",
            "Epoch: 46, Loss: 0.3556338918140477\n",
            "Epoch: 47, Loss: 0.3403144643625335\n",
            "Epoch: 48, Loss: 0.3271006937222103\n",
            "Epoch: 49, Loss: 0.31172763152271893\n",
            "Epoch: 50, Loss: 0.2982119942450767\n",
            "Epoch: 51, Loss: 0.28531603360801094\n",
            "Epoch: 52, Loss: 0.27091704556704177\n",
            "Epoch: 53, Loss: 0.25835200745964904\n",
            "Epoch: 54, Loss: 0.24861768659804484\n",
            "Epoch: 55, Loss: 0.2368802623751828\n",
            "Epoch: 56, Loss: 0.22590455591030742\n",
            "Epoch: 57, Loss: 0.2143753231372065\n",
            "Epoch: 58, Loss: 0.2053366068398099\n",
            "Epoch: 59, Loss: 0.19780521832235023\n",
            "Epoch: 60, Loss: 0.1872861757017005\n",
            "Epoch: 61, Loss: 0.17982534301059935\n",
            "Epoch: 62, Loss: 0.17251845323444936\n",
            "Epoch: 63, Loss: 0.16496529652143987\n",
            "Epoch: 64, Loss: 0.15875161185746303\n",
            "Epoch: 65, Loss: 0.15043862571801675\n",
            "Epoch: 66, Loss: 0.14437151699305495\n",
            "Epoch: 67, Loss: 0.13751766657280495\n",
            "Epoch: 68, Loss: 0.13206682422810503\n",
            "Epoch: 69, Loss: 0.1275728200193104\n",
            "Epoch: 70, Loss: 0.12247867421116061\n",
            "Epoch: 71, Loss: 0.11686463472064194\n",
            "Epoch: 72, Loss: 0.11235454692826856\n",
            "Epoch: 73, Loss: 0.10863878956193204\n",
            "Epoch: 74, Loss: 0.10426613911891075\n",
            "Epoch: 75, Loss: 0.1009210300920031\n",
            "Epoch: 76, Loss: 0.095858330135722\n",
            "Epoch: 77, Loss: 0.09278946574253347\n",
            "Epoch: 78, Loss: 0.08893397409478417\n",
            "Epoch: 79, Loss: 0.08618480701695012\n",
            "Epoch: 80, Loss: 0.08236024732632405\n",
            "Epoch: 81, Loss: 0.08055962658370547\n",
            "Epoch: 82, Loss: 0.0773720339540859\n",
            "Epoch: 83, Loss: 0.07467258776373722\n",
            "Epoch: 84, Loss: 0.07252591699266525\n",
            "Epoch: 85, Loss: 0.06969611051366152\n",
            "Epoch: 86, Loss: 0.06793684823452817\n",
            "Epoch: 87, Loss: 0.0660090964154133\n",
            "Epoch: 88, Loss: 0.06344681500416735\n",
            "Epoch: 89, Loss: 0.061759137527069165\n",
            "Epoch: 90, Loss: 0.05981962538093252\n",
            "Epoch: 91, Loss: 0.058063953614715114\n",
            "Epoch: 92, Loss: 0.05601920645274317\n",
            "Epoch: 93, Loss: 0.05473134205307421\n",
            "Epoch: 94, Loss: 0.05339515934724485\n",
            "Epoch: 95, Loss: 0.05173416161800132\n",
            "Epoch: 96, Loss: 0.05046295602103252\n",
            "Epoch: 97, Loss: 0.0492819823238932\n",
            "Epoch: 98, Loss: 0.04774151826539384\n",
            "Epoch: 99, Loss: 0.04679521308054247\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "cifar10 = datasets.CIFAR10('data', train=True, download=True)\n",
        "cifar10_val = datasets.CIFAR10('data', train=False, download=True)\n",
        "\n",
        "cifar10 = datasets.CIFAR10('data', train=True, download=False,\n",
        "                          transform=transforms.Compose([\n",
        "                              transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                                                   (0.2470, 0.2435, 0.2616))\n",
        "                          ]))\n",
        "cifar10_val = datasets.CIFAR10('data', train=False, download=False,\n",
        "                          transform=transforms.Compose([\n",
        "                              transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                                                   (0.2470, 0.2435, 0.2616))\n",
        "                          ]))\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(cifar10, batch_size=64, shuffle=True)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3072, 512),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(512, 10),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "n_epochs = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs.view(imgs.size(0), -1))\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Loss: {average_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qacJRszfnSE4",
        "outputId": "10cbe5c2-d368-4bbb-d064-f0ed9f308dd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.999500\n",
            "Valadation Accuracy: 0.472500\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Training Accuracy: %f\" % (correct / total))\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Valadation Accuracy: %f\" % (correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFwpPoya02id",
        "outputId": "c10ecc26-7574-47c8-8aee-72eb4299ba9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.923782487659503\n",
            "Epoch: 1, Loss: 1.7520965206653565\n",
            "Epoch: 2, Loss: 1.6887151879422806\n",
            "Epoch: 3, Loss: 1.6452918494753825\n",
            "Epoch: 4, Loss: 1.6076813706046784\n",
            "Epoch: 5, Loss: 1.5716648647547378\n",
            "Epoch: 6, Loss: 1.5352031707458789\n",
            "Epoch: 7, Loss: 1.4978279855550098\n",
            "Epoch: 8, Loss: 1.4597913235654612\n",
            "Epoch: 9, Loss: 1.4210287818823324\n",
            "Epoch: 10, Loss: 1.3812087243780151\n",
            "Epoch: 11, Loss: 1.3399973060468884\n",
            "Epoch: 12, Loss: 1.29718015901268\n",
            "Epoch: 13, Loss: 1.2526374611708209\n",
            "Epoch: 14, Loss: 1.2062965711516798\n",
            "Epoch: 15, Loss: 1.158098748958934\n",
            "Epoch: 16, Loss: 1.1078443578289598\n",
            "Epoch: 17, Loss: 1.055280777499499\n",
            "Epoch: 18, Loss: 1.0005419888459812\n",
            "Epoch: 19, Loss: 0.9441218706957825\n",
            "Epoch: 20, Loss: 0.8867361770032922\n",
            "Epoch: 21, Loss: 0.8275350300628511\n",
            "Epoch: 22, Loss: 0.7705323363051695\n",
            "Epoch: 23, Loss: 0.7260900350559093\n",
            "Epoch: 24, Loss: 0.6741226520341681\n",
            "Epoch: 25, Loss: 0.6153371513575849\n",
            "Epoch: 26, Loss: 0.5625666828106737\n",
            "Epoch: 27, Loss: 0.5142098669239017\n",
            "Epoch: 28, Loss: 0.4530149759234065\n",
            "Epoch: 29, Loss: 0.42537659599119443\n",
            "Epoch: 30, Loss: 0.39161583155279267\n",
            "Epoch: 31, Loss: 0.3356025654898809\n",
            "Epoch: 32, Loss: 0.30252358823766945\n",
            "Epoch: 33, Loss: 0.2954566365660495\n",
            "Epoch: 34, Loss: 0.24063307133591388\n",
            "Epoch: 35, Loss: 0.21983295506404718\n",
            "Epoch: 36, Loss: 0.19110669612960743\n",
            "Epoch: 37, Loss: 0.1764378387509557\n",
            "Epoch: 38, Loss: 0.1603739225803434\n",
            "Epoch: 39, Loss: 0.15584830106105035\n",
            "Epoch: 40, Loss: 0.14492004297440275\n",
            "Epoch: 41, Loss: 0.11330997701519933\n",
            "Epoch: 42, Loss: 0.09211532989054766\n",
            "Epoch: 43, Loss: 0.0814558459958538\n",
            "Epoch: 44, Loss: 0.07310723094537597\n",
            "Epoch: 45, Loss: 0.06871523767741888\n",
            "Epoch: 46, Loss: 0.056692555906427335\n",
            "Epoch: 47, Loss: 0.0856771266148628\n",
            "Epoch: 48, Loss: 0.05805049207814209\n",
            "Epoch: 49, Loss: 0.05019129839811541\n",
            "Epoch: 50, Loss: 0.03584716502142965\n",
            "Epoch: 51, Loss: 0.06030706873418444\n",
            "Epoch: 52, Loss: 0.043952478534754015\n",
            "Epoch: 53, Loss: 0.02441924791205484\n",
            "Epoch: 54, Loss: 0.047931856823974356\n",
            "Epoch: 55, Loss: 0.023282848692753964\n",
            "Epoch: 56, Loss: 0.016277340913216448\n",
            "Epoch: 57, Loss: 0.04183039311265997\n",
            "Epoch: 58, Loss: 0.013002517030668466\n",
            "Epoch: 59, Loss: 0.01141803549490083\n",
            "Epoch: 60, Loss: 0.009165939914025104\n",
            "Epoch: 61, Loss: 0.006698605889255595\n",
            "Epoch: 62, Loss: 0.0056185993071406716\n",
            "Epoch: 63, Loss: 0.00495111033626501\n",
            "Epoch: 64, Loss: 0.004440692576083719\n",
            "Epoch: 65, Loss: 0.004082192622428543\n",
            "Epoch: 66, Loss: 0.0037923062440839207\n",
            "Epoch: 67, Loss: 0.0035472255673698956\n",
            "Epoch: 68, Loss: 0.003336338660917352\n",
            "Epoch: 69, Loss: 0.003151952232549186\n",
            "Epoch: 70, Loss: 0.00298889681594619\n",
            "Epoch: 71, Loss: 0.0028434030669148833\n",
            "Epoch: 72, Loss: 0.0027125861852162142\n",
            "Epoch: 73, Loss: 0.0025941898314686624\n",
            "Epoch: 74, Loss: 0.002486414934174739\n",
            "Epoch: 75, Loss: 0.002387806798921406\n",
            "Epoch: 76, Loss: 0.002297173263142606\n",
            "Epoch: 77, Loss: 0.0022135299972935564\n",
            "Epoch: 78, Loss: 0.002136053809969206\n",
            "Epoch: 79, Loss: 0.002064052183506921\n",
            "Epoch: 80, Loss: 0.0019969355186459114\n",
            "Epoch: 81, Loss: 0.0019342006315388467\n",
            "Epoch: 82, Loss: 0.0018754130875220751\n",
            "Epoch: 83, Loss: 0.00182019595803051\n",
            "Epoch: 84, Loss: 0.0017682224915869346\n",
            "Epoch: 85, Loss: 0.0017192035012365179\n",
            "Epoch: 86, Loss: 0.0016728854096216825\n",
            "Epoch: 87, Loss: 0.0016290437170330262\n",
            "Epoch: 88, Loss: 0.0015874786724378426\n",
            "Epoch: 89, Loss: 0.0015480120204757694\n",
            "Epoch: 90, Loss: 0.0015104835416199854\n",
            "Epoch: 91, Loss: 0.0014747516544069202\n",
            "Epoch: 92, Loss: 0.0014406858078922237\n",
            "Epoch: 93, Loss: 0.001408169426813262\n",
            "Epoch: 94, Loss: 0.0013770963566418252\n",
            "Epoch: 95, Loss: 0.0013473711059609776\n",
            "Epoch: 96, Loss: 0.0013189054196192812\n",
            "Epoch: 97, Loss: 0.0012916198563854308\n",
            "Epoch: 98, Loss: 0.001265440280848082\n",
            "Epoch: 99, Loss: 0.0012402998448954388\n",
            "Epoch: 100, Loss: 0.0012161364081649758\n",
            "Epoch: 101, Loss: 0.0011928932446394088\n",
            "Epoch: 102, Loss: 0.001170517601113876\n",
            "Epoch: 103, Loss: 0.0011489622808146247\n",
            "Epoch: 104, Loss: 0.0011281810848906104\n",
            "Epoch: 105, Loss: 0.0011081332592145496\n",
            "Epoch: 106, Loss: 0.0010887792196275596\n",
            "Epoch: 107, Loss: 0.0010700838338147374\n",
            "Epoch: 108, Loss: 0.0010520142815874495\n",
            "Epoch: 109, Loss: 0.0010345378481467966\n",
            "Epoch: 110, Loss: 0.0010176259815092813\n",
            "Epoch: 111, Loss: 0.0010012517862092339\n",
            "Epoch: 112, Loss: 0.0009853893086546287\n",
            "Epoch: 113, Loss: 0.0009700151690071428\n",
            "Epoch: 114, Loss: 0.0009551071650542877\n",
            "Epoch: 115, Loss: 0.0009406432502723211\n",
            "Epoch: 116, Loss: 0.0009266046096117187\n",
            "Epoch: 117, Loss: 0.0009129724733441707\n",
            "Epoch: 118, Loss: 0.0008997290864397172\n",
            "Epoch: 119, Loss: 0.000886857238709224\n",
            "Epoch: 120, Loss: 0.0008743427650820788\n",
            "Epoch: 121, Loss: 0.0008621707050915202\n",
            "Epoch: 122, Loss: 0.000850326284217095\n",
            "Epoch: 123, Loss: 0.0008387968510385905\n",
            "Epoch: 124, Loss: 0.0008275696650471823\n",
            "Epoch: 125, Loss: 0.0008166336509699979\n",
            "Epoch: 126, Loss: 0.0008059776239202815\n",
            "Epoch: 127, Loss: 0.0007955898422144337\n",
            "Epoch: 128, Loss: 0.0007854612451638458\n",
            "Epoch: 129, Loss: 0.000775582010579302\n",
            "Epoch: 130, Loss: 0.0007659426016342716\n",
            "Epoch: 131, Loss: 0.0007565341413348152\n",
            "Epoch: 132, Loss: 0.0007473497773291986\n",
            "Epoch: 133, Loss: 0.0007383802789765859\n",
            "Epoch: 134, Loss: 0.0007296188904479851\n",
            "Epoch: 135, Loss: 0.0007210580265389748\n",
            "Epoch: 136, Loss: 0.0007126912309443834\n",
            "Epoch: 137, Loss: 0.000704510997378808\n",
            "Epoch: 138, Loss: 0.0006965129221479892\n",
            "Epoch: 139, Loss: 0.0006886897818728759\n",
            "Epoch: 140, Loss: 0.0006810366881520325\n",
            "Epoch: 141, Loss: 0.0006735462247377055\n",
            "Epoch: 142, Loss: 0.0006662146363332493\n",
            "Epoch: 143, Loss: 0.0006590380010207283\n",
            "Epoch: 144, Loss: 0.0006520105236766847\n",
            "Epoch: 145, Loss: 0.0006451271580616453\n",
            "Epoch: 146, Loss: 0.0006383834286335954\n",
            "Epoch: 147, Loss: 0.0006317750073607911\n",
            "Epoch: 148, Loss: 0.0006252990777498138\n",
            "Epoch: 149, Loss: 0.0006189507236724328\n",
            "Epoch: 150, Loss: 0.0006127268573871571\n",
            "Epoch: 151, Loss: 0.000606623642147555\n",
            "Epoch: 152, Loss: 0.000600636905703671\n",
            "Epoch: 153, Loss: 0.0005947640373544527\n",
            "Epoch: 154, Loss: 0.0005890022732966962\n",
            "Epoch: 155, Loss: 0.0005833474061195203\n",
            "Epoch: 156, Loss: 0.0005777970083420227\n",
            "Epoch: 157, Loss: 0.0005723488756840009\n",
            "Epoch: 158, Loss: 0.0005669983558125569\n",
            "Epoch: 159, Loss: 0.0005617448624920622\n",
            "Epoch: 160, Loss: 0.0005565854399252143\n",
            "Epoch: 161, Loss: 0.0005515159523348882\n",
            "Epoch: 162, Loss: 0.0005465364354681801\n",
            "Epoch: 163, Loss: 0.000541642952241156\n",
            "Epoch: 164, Loss: 0.0005368333883368282\n",
            "Epoch: 165, Loss: 0.0005321068113626283\n",
            "Epoch: 166, Loss: 0.0005274595349574225\n",
            "Epoch: 167, Loss: 0.0005228900982354604\n",
            "Epoch: 168, Loss: 0.000518397196464951\n",
            "Epoch: 169, Loss: 0.0005139786132235888\n",
            "Epoch: 170, Loss: 0.0005096318863812819\n",
            "Epoch: 171, Loss: 0.0005053557393982496\n",
            "Epoch: 172, Loss: 0.0005011492136438154\n",
            "Epoch: 173, Loss: 0.000497009399730493\n",
            "Epoch: 174, Loss: 0.000492935673935655\n",
            "Epoch: 175, Loss: 0.0004889261185234297\n",
            "Epoch: 176, Loss: 0.0004849788846748481\n",
            "Epoch: 177, Loss: 0.0004810932086190552\n",
            "Epoch: 178, Loss: 0.00047726728987288744\n",
            "Epoch: 179, Loss: 0.00047349952646028585\n",
            "Epoch: 180, Loss: 0.00046978870377084776\n",
            "Epoch: 181, Loss: 0.0004661335911907856\n",
            "Epoch: 182, Loss: 0.00046253383563146056\n",
            "Epoch: 183, Loss: 0.00045898689603445815\n",
            "Epoch: 184, Loss: 0.0004554923387266525\n",
            "Epoch: 185, Loss: 0.0004520486990540542\n",
            "Epoch: 186, Loss: 0.0004486554014437553\n",
            "Epoch: 187, Loss: 0.00044531107066533246\n",
            "Epoch: 188, Loss: 0.00044201437148752045\n",
            "Epoch: 189, Loss: 0.0004387648019204175\n",
            "Epoch: 190, Loss: 0.00043556030764112155\n",
            "Epoch: 191, Loss: 0.00043240154369981646\n",
            "Epoch: 192, Loss: 0.0004292861458103624\n",
            "Epoch: 193, Loss: 0.00042621394669551635\n",
            "Epoch: 194, Loss: 0.00042318381038941756\n",
            "Epoch: 195, Loss: 0.00042019585987621927\n",
            "Epoch: 196, Loss: 0.000417247013573873\n",
            "Epoch: 197, Loss: 0.00041433852799931993\n",
            "Epoch: 198, Loss: 0.0004114692317652534\n",
            "Epoch: 199, Loss: 0.0004086380841399101\n",
            "Epoch: 200, Loss: 0.00040584436135159333\n",
            "Epoch: 201, Loss: 0.000403086823677612\n",
            "Epoch: 202, Loss: 0.0004003655502695919\n",
            "Epoch: 203, Loss: 0.00039767957339725753\n",
            "Epoch: 204, Loss: 0.00039502804569532774\n",
            "Epoch: 205, Loss: 0.0003924100849632462\n",
            "Epoch: 206, Loss: 0.00038982549368481563\n",
            "Epoch: 207, Loss: 0.0003872736861501002\n",
            "Epoch: 208, Loss: 0.00038475416204862323\n",
            "Epoch: 209, Loss: 0.00038226590967336535\n",
            "Epoch: 210, Loss: 0.0003798084532446198\n",
            "Epoch: 211, Loss: 0.0003773814918382702\n",
            "Epoch: 212, Loss: 0.0003749841403787333\n",
            "Epoch: 213, Loss: 0.00037261630195713886\n",
            "Epoch: 214, Loss: 0.00037027655076957546\n",
            "Epoch: 215, Loss: 0.0003679652812762235\n",
            "Epoch: 216, Loss: 0.00036568175341822014\n",
            "Epoch: 217, Loss: 0.000363425470885935\n",
            "Epoch: 218, Loss: 0.00036119612254378923\n",
            "Epoch: 219, Loss: 0.00035899259461201027\n",
            "Epoch: 220, Loss: 0.00035681522103565264\n",
            "Epoch: 221, Loss: 0.000354662681307496\n",
            "Epoch: 222, Loss: 0.0003525348644691866\n",
            "Epoch: 223, Loss: 0.000350432105284557\n",
            "Epoch: 224, Loss: 0.0003483533454832503\n",
            "Epoch: 225, Loss: 0.0003462980389171873\n",
            "Epoch: 226, Loss: 0.0003442658210906651\n",
            "Epoch: 227, Loss: 0.0003422569651637882\n",
            "Epoch: 228, Loss: 0.0003402701702567435\n",
            "Epoch: 229, Loss: 0.0003383056569045506\n",
            "Epoch: 230, Loss: 0.00033636275574236977\n",
            "Epoch: 231, Loss: 0.00033444068212051104\n",
            "Epoch: 232, Loss: 0.0003325411092300969\n",
            "Epoch: 233, Loss: 0.0003306611874404669\n",
            "Epoch: 234, Loss: 0.00032880144803609123\n",
            "Epoch: 235, Loss: 0.00032696238690325507\n",
            "Epoch: 236, Loss: 0.00032514273672273246\n",
            "Epoch: 237, Loss: 0.00032334265360196184\n",
            "Epoch: 238, Loss: 0.0003215612729813274\n",
            "Epoch: 239, Loss: 0.0003197989188782026\n",
            "Epoch: 240, Loss: 0.0003180551996469558\n",
            "Epoch: 241, Loss: 0.000316329920883676\n",
            "Epoch: 242, Loss: 0.00031462223925649355\n",
            "Epoch: 243, Loss: 0.0003129323417258844\n",
            "Epoch: 244, Loss: 0.00031126002601650514\n",
            "Epoch: 245, Loss: 0.0003096044413204861\n",
            "Epoch: 246, Loss: 0.00030796562288714366\n",
            "Epoch: 247, Loss: 0.00030634393861289985\n",
            "Epoch: 248, Loss: 0.00030473878695191117\n",
            "Epoch: 249, Loss: 0.0003031492552553203\n",
            "Epoch: 250, Loss: 0.00030157555765214066\n",
            "Epoch: 251, Loss: 0.00030001839665375806\n",
            "Epoch: 252, Loss: 0.00029847574959512943\n",
            "Epoch: 253, Loss: 0.0002969482579096051\n",
            "Epoch: 254, Loss: 0.0002954370368237752\n",
            "Epoch: 255, Loss: 0.000293939326912106\n",
            "Epoch: 256, Loss: 0.0002924567822593531\n",
            "Epoch: 257, Loss: 0.00029098782020982306\n",
            "Epoch: 258, Loss: 0.00028953373679113984\n",
            "Epoch: 259, Loss: 0.00028809308407843395\n",
            "Epoch: 260, Loss: 0.00028666651816862247\n",
            "Epoch: 261, Loss: 0.0002852530270895136\n",
            "Epoch: 262, Loss: 0.0002838533766123855\n",
            "Epoch: 263, Loss: 0.00028246634593017604\n",
            "Epoch: 264, Loss: 0.0002810925848175348\n",
            "Epoch: 265, Loss: 0.00027973225923049496\n",
            "Epoch: 266, Loss: 0.0002783836160866169\n",
            "Epoch: 267, Loss: 0.0002770481277903666\n",
            "Epoch: 268, Loss: 0.00027572438685798687\n",
            "Epoch: 269, Loss: 0.0002744134724712344\n",
            "Epoch: 270, Loss: 0.00027311368334002\n",
            "Epoch: 271, Loss: 0.0002718263703686914\n",
            "Epoch: 272, Loss: 0.00027055009242451345\n",
            "Epoch: 273, Loss: 0.0002692856144972037\n",
            "Epoch: 274, Loss: 0.0002680328671488187\n",
            "Epoch: 275, Loss: 0.00026679072963067\n",
            "Epoch: 276, Loss: 0.00026555938054797807\n",
            "Epoch: 277, Loss: 0.00026433960134510364\n",
            "Epoch: 278, Loss: 0.00026313010546619365\n",
            "Epoch: 279, Loss: 0.00026193163329576107\n",
            "Epoch: 280, Loss: 0.0002607437287326804\n",
            "Epoch: 281, Loss: 0.00025956580185682287\n",
            "Epoch: 282, Loss: 0.00025839828116347495\n",
            "Epoch: 283, Loss: 0.00025724062250032835\n",
            "Epoch: 284, Loss: 0.00025609308145167677\n",
            "Epoch: 285, Loss: 0.00025495556704478923\n",
            "Epoch: 286, Loss: 0.00025382751564881876\n",
            "Epoch: 287, Loss: 0.00025270868467291773\n",
            "Epoch: 288, Loss: 0.00025159940785279405\n",
            "Epoch: 289, Loss: 0.0002504995734159685\n",
            "Epoch: 290, Loss: 0.0002494092343038718\n",
            "Epoch: 291, Loss: 0.00024832777948028084\n",
            "Epoch: 292, Loss: 0.00024725527121418077\n",
            "Epoch: 293, Loss: 0.00024619128011242077\n",
            "Epoch: 294, Loss: 0.00024513666638485937\n",
            "Epoch: 295, Loss: 0.00024409059926972735\n",
            "Epoch: 296, Loss: 0.00024305291004761634\n",
            "Epoch: 297, Loss: 0.00024202347565528668\n",
            "Epoch: 298, Loss: 0.0002410028637569967\n",
            "Epoch: 299, Loss: 0.00023999034868297375\n"
          ]
        }
      ],
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(3072, 1024),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(1024, 512),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(512, 128),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(128, 10),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "n_epochs = 300\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs.view(imgs.size(0), -1))\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Loss: {average_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Training Accuracy: %f\" % (correct / total))\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Valadation Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH9KUp4jmH78",
        "outputId": "314134ba-98a2-48b8-bd23-37e8e7cc1d15"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 1.000000\n",
            "Valadation Accuracy: 0.463800\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi8TC01z1C1HHwaigUCfpq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}